# run_evaluation.py

import json
import os
from bootstrap import parse_args, initialize_components
from src.loaders.base_loader import PageElement
from src.utils.llm_helper import create_llm_caller

def main():
    args = parse_args()
    print(f"ğŸš€ Starting Evaluation Stage for {args.benchmark} (Task: {args.evaluation_task})...")
    
    # åˆå§‹åŒ– Loader (ä¸éœ€è¦åŠ è½½ heavy models)
    _, loader = initialize_components(args, init_retriever=False, init_generator=False)
    loader.llm_caller = create_llm_caller()
    
    # 1. ç¡®å®šè¾“å…¥æ–‡ä»¶
    input_file = args.evaluation_input
    if input_file is None:
        if args.evaluation_task == "retrieval":
            # ä¼˜å…ˆæ‰¾ retrieval_results.jsonï¼Œå¦‚æœæ²¡æœ‰åˆ™æ‰¾ generation_results.json
            p1 = os.path.join(args.output_dir, "retrieval_results.json")
            p2 = os.path.join(args.output_dir, "generation_results.json")
            input_file = p1 if os.path.exists(p1) else p2
        else:
            # Generation æˆ– All å¿…é¡»ç”¨ generation_results.json
            input_file = os.path.join(args.output_dir, "generation_results.json")
    else:
        input_file = os.path.join(args.output_dir, input_file)
    
    if not input_file or not os.path.exists(input_file):
        print(f"âŒ Error: Input file not found: {input_file}")
        return

    print(f"ğŸ“‚ Loading results from: {input_file}")
    with open(input_file, 'r', encoding='utf-8') as f:
        results_data = json.load(f)
        
    # 2. å°†ç»“æœæ˜ å°„å› Loader çš„ samples
    # å»ºç«‹æ˜ å°„è¡¨
    results_map = {item['qid']: item for item in results_data}
    
    matched_count = 0
    for sample in loader.samples:
        if sample.qid in results_map:
            res = results_map[sample.qid]
            if sample.extra_info is None:
                sample.extra_info = {}
            
            # æ³¨å…¥æ£€ç´¢ç»“æœ (å¦‚æœå­˜åœ¨)
            if 'retrieved_elements' in res:
                sample.extra_info['retrieved_elements'] = res['retrieved_elements']
            
            # æ³¨å…¥ç”Ÿæˆç»“æœ (å¦‚æœå­˜åœ¨)
            if 'model_answer' in res:
                sample.extra_info['final_answer'] = res['model_answer']
            elif 'final_answer' in res: # å…¼å®¹æ—§æ ¼å¼
                sample.extra_info['final_answer'] = res['final_answer']

            matched_count += 1
            
    print(f"âœ… Mapped results for {matched_count}/{len(loader.samples)} samples.")
    
    final_metrics = {}

    # 3. æ‰§è¡Œè¯„ä¼°
    # Task: Retrieval
    if args.evaluation_task in ["retrieval", "all"]:
        try:
            print("\n--- Retrieval Metrics ---")
            r_metrics = loader.evaluate_retrieval()
            print(json.dumps(r_metrics, indent=2))
            final_metrics.update(r_metrics)
        except Exception as e:
            print(f"âš ï¸ Retrieval evaluation failed: {e}")

    # Task: Generation
    if args.evaluation_task in ["generation", "all"]:
        # æ£€æŸ¥æ˜¯å¦å…·å¤‡ç”Ÿæˆç»“æœ
        has_answers = any("final_answer" in s.extra_info for s in loader.samples if s.qid in results_map)
        if has_answers:
            try:
                print("\n--- Generation Metrics ---")
                g_metrics = loader.evaluate_generation()
                print(json.dumps(g_metrics, indent=2))
                final_metrics.update(g_metrics)
            except Exception as e:
                print(f"âš ï¸ Generation evaluation failed: {e}")
        else:
            if args.evaluation_task == "generation":
                print("âš ï¸ Warning: No generation answers found in input file. Skipping generation eval.")

    # 4. ä¿å­˜è¯„ä¼°æŠ¥å‘Š
    output_path = os.path.join(args.output_dir, f"evaluation_metrics_{args.evaluation_task}.json")
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(final_metrics, f, indent=2)
    print(f"\nğŸ’¾ All metrics saved to {output_path}")

if __name__ == "__main__":
    main()